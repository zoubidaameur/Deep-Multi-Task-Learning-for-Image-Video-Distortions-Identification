{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_MTL_distortion_identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WqA6lXb4r8xA8o76ElfEg_DzEnquIoLU",
      "authorship_tag": "ABX9TyM8ULrVAjvHV17k7rt6kDwM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoubidaameur/Deep-Multi-Task-Learning-for-Image-Video-Distortions-Identification/blob/main/Deep_MTL_distortion_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QNHRDkmO6Uk"
      },
      "source": [
        "### **Deep Multi-Task Learning for Image/Video Distortions Identification**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2s3cKD5PN97"
      },
      "source": [
        "This notebook demonstrates image distortions identification using deep multi-task learning. Using this technique you can identify and classify several distortion types using a single model simultaneously and accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv7Jy5UDPumP"
      },
      "source": [
        "### **Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPkfis4EP1P-"
      },
      "source": [
        "import os, sys\n",
        "import pickle\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import applications \n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from skimage.util import view_as_windows\n",
        "from tensorflow.keras.layers import MaxPooling2D ,Dense ,Dropout, Flatten\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.applications.densenet import DenseNet169\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback,TensorBoard\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KdJFRxvRawj"
      },
      "source": [
        "### **Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uibeIGmRgCp"
      },
      "source": [
        "############## Uncomment the section of the desired dataset #####################\n",
        "\n",
        "\n",
        "# ####### TID-2013 #########\n",
        "# !wget \"http://www.ponomarenko.info/tid2013/tid2013.rar\"\n",
        "# !pip install unrar\n",
        "# !unrar x \"/content/tid2013.rar\"\n",
        "\n",
        "\n",
        "####### KADID-10K #########\n",
        "# !wget \"https://datasets.vqa.mmsp-kn.de/archives/kadid10k.zip\"\n",
        "# !unzip /content/kadid10k.zip\n",
        "\n",
        "####### CSIQ #########*\n",
        "# !wget \"http://vision.eng.shizuoka.ac.jp/csiq/dst_imgs.zip\"\n",
        "# !unzip /content/dst_imgs.zip\n",
        "\n",
        "\n",
        "# ####### LIVEMD #########*\n",
        "# !wget https://public.boxcloud.com/d/1/b1!IU4S1kNcRl9668x9nt0yijL48I6EGcI3qmccUX2YNXfVw4O2LNS4fEyI3x5aNXOL2OZWHNt-Z7vTEijwPWtsasa8_P2sdaE44u-7QR1N6cNOC3afB8Szq4biRIvtRNmLTnom6NZfdFNQSMbjG6g2yTbPpRoE1YuEGIT648tUedT_eDMHGEPDyINX4hOPrRV1CvIDYMqR4K7Oa0TrM689E8nF-RDRTH2ijx0PSDc84TxdORQ79XRpIq59K3-1OEkLvnDrpcPLxsZXiZAHNjrjggCjYNscJ83COC3_JUWgR6RQ_GpvoyB_60ba1b6o76mQ1UbFRnJ3snPCEuTxb_396uRdq4tEWrnf4G-dn5NKLdvofFiuXfFFEssLoRk3beeY10EuU7z-z6w2sB_3bgJnMysFwUleBBmEgk7zbizL6rtqZ6jxcRhzmGFD7JubS8sP_nQOrIo9JMbf95oMIfLQsom7A1LlgoSyeHJ23QTQuS1Syzjo7_iHE98jBJV3LgSRGsRLPCLfgbCEFmAQEWZ5qIHETx9FEsHtPMrCB8elqLLpfzYbRf5yq8_75sM7pn_Z2ardTDAOa_Uot-nP_rqVMTCHcJSjDPygX7wNwiGITIIQZrL5zX5LUXzCmRdGjgOCuATUIOKniWKrPRmd4lowJ5kJMHM4Gd87OifblNSMxxH1jyaViMT9z5cH8kPl2Eybl3SHTsmbiernufEAnKBaHk4YjEFFTgDaNMklhclyobRNiXJaem5IiO7qmgNhgOhbT0yAYPswQA19ufH0YHzEdEuXuQ1GW2ZqUsY-kojA_0Lz2kt-kWYgIUG8UpopB_YuB64n96icArMIbbDKdbXnsZVu0myUuEMOTkgMTWKsiCYsinvBwOD4b0l1kDtkBAsQToEU34us_xyloT_A1PoV_E6bKJPTdeaTpryWWe-RSM6T5eVnKWlRZqa73-T5WqMIkGLFTdRWJTGs1VlT_GZdFCqPR-VLg6jYvZQ7ju7Bps9defkzJ-r5VmZFEV6LFultyy2tUIEaNvRi6V1YNBHRGfTh-KBaN00B6wNaQcM1FprBRiTN6jixzInXuKTFTOnS7KxiarHX_7x1P19qlf9XU61puHYJIsTQU5CyjA2n5c__JpyQIx94cOykBtle4tA88m8V4CGBTFsNRqZoNpCC1rqh-r-gTiQKz7Tbc8ea4MQiIVd0TUAKFiXge2Thczjgl-_n0nD3M7dO3FiOZvAQNUxEonDGfjsoYGtMNHgoKQO6fH2y68joH26PDr0bdeSdW820M0N50DXIu4p8OJZRNB8mWcGqrCDWXEjq/download\n",
        "# !unrar x /content/drive/MyDrive/download -plivemultidistortiondatabase2013\n",
        "# mkdir /content/livemd\n",
        "# !mv /content/livemd/To_Release/Part1/blurjpeg/* /content/livemd/\n",
        "# !mv /content/livemd/To_Release/Part2/blurnoise/* /content/livemd/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYzNhLuPTPFi"
      },
      "source": [
        "### **Data generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1cl4uA-TVMo"
      },
      "source": [
        "class generator_overlapping(tensorflow.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self):\n",
        "        self.on_epoch_end()\n",
        "    'Denotes the number of batches per epoch'\n",
        "    def __len__(self):\n",
        "      \n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def load_pkl(self,list_IDs_path,labels_path1,labels_path2,labels_path3, part):\n",
        "        pickle_in = open(list_IDs_path,'rb')\n",
        "        list_IDs = pickle.load(pickle_in)[part]\n",
        "        pickle_in.close()\n",
        "\n",
        "        pickle_in2 = open(labels_path1,'rb')\n",
        "        labels1 = pickle.load(pickle_in2)\n",
        "        pickle_in2.close()\n",
        "        \n",
        "        pickle_in2 = open(labels_path2,'rb')\n",
        "        labels2 = pickle.load(pickle_in2)\n",
        "        pickle_in2.close()\n",
        "        \n",
        "        pickle_in2 = open(labels_path3,'rb')\n",
        "        labels3 = pickle.load(pickle_in2)\n",
        "        pickle_in2.close()\n",
        "\n",
        "        return  list_IDs, labels1, labels2, labels3\n",
        "    \n",
        "    def loading_img(self):\n",
        "        return image.load_img(self.db_path+self.ID)\n",
        "\n",
        "    def init_y(self):\n",
        "        return np.empty((self.patches*self.batch_size,1), dtype=np.float32)\n",
        "    \n",
        "    def update_y1(self,ID):\n",
        "        return self.labels1[ID]\n",
        "    def update_y2(self,ID):\n",
        "        return self.labels2[ID]\n",
        "    def update_y3(self,ID):\n",
        "        return self.labels3[ID]\n",
        "    \n",
        "    \n",
        "    def update_x(self,x):\n",
        "        return x \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        # Generate data\n",
        "        X, y1, y2, y3 = self.__data_generation(list_IDs_temp)\n",
        "        return X, [y1, y2, y3]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def ajust(self,img):\n",
        "        return img\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' \n",
        "        # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        \n",
        "        X = np.empty((self.patches*self.batch_size, *self.dim, self.n_channels))\n",
        "        y1 =self.init_y() \n",
        "        y2 =self.init_y() \n",
        "        y3 = self.init_y()\n",
        "       \n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            self.ID=ID    \n",
        "            img = image.load_img(self.db_path+ID)\n",
        "            img = image.img_to_array(img)\n",
        "            img = applications.densenet.preprocess_input(img)\n",
        "            img=self.ajust(img)\n",
        "            x=view_as_windows(np.ascontiguousarray(img),(*self.dim,3),self.overlap_stride).reshape((-1,*self.dim,3))      \n",
        "\n",
        "            X[(i)*self.patches :(i+1)*self.patches,:,:,:]=self.update_x(x)\n",
        "            y1[(i)*self.patches :(i+1)*self.patches]=self.update_y1(ID)\n",
        "            y2[(i)*self.patches :(i+1)*self.patches]=self.update_y2(ID)\n",
        "            y3[(i)*self.patches :(i+1)*self.patches]=self.update_y3(ID)\n",
        "        return X, y1, y2, y3\n",
        "\n",
        "   \n",
        "    \n",
        "class LIVEMD_GENERATOR(generator_overlapping):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\n",
        "        self.base=base\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.n_output = n_output\n",
        "        self.shuffle = shuffle\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\n",
        "        self.db_path='/content/livemd/'\n",
        "        list_IDs_path='/content/partition_livemd.pickle'\n",
        "        labels_path1='/content/blur_livemd.pickle'\n",
        "        labels_path2='/content/jpeg_livemd.pickle'\n",
        "        labels_path3 = '/content/noise_livemd.pickle'\n",
        "        self.patches=8  \n",
        "        self.overlap_stride = 350\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\n",
        "        super().__init__()\n",
        "\n",
        "class TID_GENERATOR(generator_overlapping):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\n",
        "        self.base=base\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.n_output = n_output\n",
        "        self.shuffle = shuffle\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\n",
        "        self.db_path='/content/distorted_images/'\n",
        "        list_IDs_path='/content/partition_tid.pickle'\n",
        "        labels_path1='/content/blur_tid.pickle'\n",
        "        labels_path2='/content/jpeg_tid.pickle'\n",
        "        labels_path3 = '/content/noise_tid.pickle'\n",
        "        self.patches=4  \n",
        "        self.overlap_stride = 150\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\n",
        "        super().__init__()\n",
        "\n",
        "class CSIQ_GENERATOR(generator_overlapping):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\n",
        "        self.base=base\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.n_output = n_output\n",
        "        self.shuffle = shuffle\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\n",
        "        self.db_path='/content/dst_imgs/'\n",
        "        list_IDs_path='/content/partition_csiq.pickle'\n",
        "        labels_path1='/content/blur_csiq.pickle'\n",
        "        labels_path2='/content/jpeg_csiq.pickle'\n",
        "        labels_path3 = '/content/noise_csiq.pickle'\n",
        "        self.patches=4  \n",
        "        self.overlap_stride = 160\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\n",
        "        super().__init__()\n",
        "\n",
        "class KADID_GENERATOR(generator_overlapping):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\n",
        "        self.base=base\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.n_output = n_output\n",
        "        self.shuffle = shuffle\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\n",
        "        self.db_path='/content/kadid10k/images/'\n",
        "        list_IDs_path='/content/partition_kadid.pickle'\n",
        "        labels_path1='/content/blur_kadid.pickle'\n",
        "        labels_path2='/content/jpeg_kadid.pickle'\n",
        "        labels_path3 = '/content/noise_kadid.pickle'\n",
        "        self.patches=6  \n",
        "        self.overlap_stride = 100\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9hTh0EdU_MH"
      },
      "source": [
        "### **Build  model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YvhIqQdVEpq"
      },
      "source": [
        "def build_model(max_pool= False ,weights='imagenet', dropOutRate=0.25,hiddenLayerDim=512,num_denseLayer=2,input_shape = (224,224,3), include_top = False,fine_tune_all = False, num_towers =2): \n",
        "    \n",
        "    base_model = DenseNet169(weights=weights ,include_top = include_top, input_shape = input_shape)\n",
        "\n",
        "    if (fine_tune_all ==False):\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "            \n",
        "    x =base_model.layers[-1].output\n",
        "        \n",
        "    if (max_pool):\n",
        "        x= MaxPooling2D(pool_size=(2,2))(x)\n",
        "    x =Flatten()(x)\n",
        "    features = x\n",
        "    for i in range(num_denseLayer):\n",
        "        features = Dense(hiddenLayerDim, activation='relu',name=\"DenseTower1\"+str(i))(features)\n",
        "        features = Dropout(dropOutRate, name=\"DropoutTower1\"+ str(i))(features)\n",
        "    output1 = Dense(1, name=\"Tower1\", activation=\"sigmoid\")(features)\n",
        "        \n",
        "       \n",
        "    features = x\n",
        "    for i in range(num_denseLayer):\n",
        "        features = Dense(hiddenLayerDim, activation='relu',name=\"DenseTower2\"+str(i))(features)\n",
        "        features = Dropout(dropOutRate, name=\"DropoutTower2\"+ str(i))(features)\n",
        "    output2 = Dense(1, name= \"Tower2\", activation=\"sigmoid\")(features)\n",
        "             \n",
        "    features = x\n",
        "    for i in range(num_denseLayer):\n",
        "        features = Dense(hiddenLayerDim, activation='relu',name=\"DenseTower3\"+str(i))(features)\n",
        "        features = Dropout(dropOutRate, name=\"DropoutTower3\"+ str(i))(features)\n",
        "    output3 = Dense(1, name= \"Tower3\", activation=\"sigmoid\")(features)\n",
        "        \n",
        "\n",
        "\n",
        "    model = Model(inputs=base_model.layers[0].output, outputs= [output1, output2, output3])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vs-yR68V2fT"
      },
      "source": [
        "### **Train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyzrsI1ZV-VO"
      },
      "source": [
        "def Training(batch_size=8,db='CSIQ',dropOutRate=0.25,hiddenLayerDim=512,epochs=60, num_denseLayer=2,fine_tune_all=False):\n",
        "\n",
        "\n",
        "    #Define parameters \n",
        "    params = {'dim':(224,224),\n",
        "        'batch_size': batch_size,\n",
        "        'n_output': 1,\n",
        "        'n_channels': 3,\n",
        "        'shuffle': True,\n",
        "    }\n",
        "\n",
        "    if (db=='TID'):\n",
        "        training_generator = TID_GENERATOR(part='train', **params)\n",
        "        validation_generator =TID_GENERATOR(part='test', **params)\n",
        "\n",
        "    if (db=='CSIQ'):\n",
        "        training_generator = CSIQ_GENERATOR(part='train', **params)\n",
        "        validation_generator = CSIQ_GENERATOR(part='test', **params)\n",
        "\n",
        "    if (db=='KADID'):\n",
        "        training_generator = KADID_GENERATOR(part='train', **params)\n",
        "        validation_generator = KADID_GENERATOR(part='test', **params)\n",
        "\n",
        "    if (db=='LIVEMD'):\n",
        "        training_generator = LIVEMD_GENERATOR(part='train', **params)\n",
        "        validation_generator = LIVEMD_GENERATOR(part='test', **params)        \n",
        "\n",
        "\n",
        "    model = build_model (weights='imagenet',dropOutRate=0.25,hiddenLayerDim=512,num_denseLayer=2, input_shape=(224, 224, 3),fine_tune_all= False, max_pool = True)\n",
        "    model.summary()\n",
        "    adam=Adam(lr=0.0001)\n",
        "    losses = {\n",
        "            'Tower1': 'binary_crossentropy',\n",
        "            'Tower2': 'binary_crossentropy',\n",
        "            'Tower3': 'binary_crossentropy'\n",
        "    }\n",
        "    lossWeights = {\n",
        "            'Tower1': 1.0, \n",
        "            'Tower2': 1.0,\n",
        "            'Tower3':1.0\n",
        "    }\n",
        "    metrics = {\n",
        "            'Tower1': 'accuracy', \n",
        "            'Tower2': 'accuracy',\n",
        "            'Tower3':'accuracy'\n",
        "    }\n",
        "\n",
        "    model.compile(optimizer=adam,\n",
        "              loss= losses,\n",
        "              loss_weights=lossWeights,\n",
        "              metrics=metrics)\n",
        "\n",
        "\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,write_graph=True, write_images=False)\n",
        "    callbacks = tensorflow.keras.callbacks.ModelCheckpoint('weights.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
        "\n",
        "    history =model.fit_generator(generator=training_generator,\n",
        "                                validation_data=validation_generator,\n",
        "                                use_multiprocessing=True,\n",
        "                                workers=4,\n",
        "                                epochs=epochs,\n",
        "                                callbacks= callbacks\n",
        "                               )\n",
        "\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "Training(db='TID',batch_size=6, hiddenLayerDim=512,num_denseLayer=2,dropOutRate=0.25 ,fine_tune_all=False ,epochs=60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QVzjNiYX1wL"
      },
      "source": [
        "### **Test model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFjStau8X1NO"
      },
      "source": [
        "def predictions(patches= 4 ,partition_path='',labels_path1='',labels_path2='',labels_path3='',y_pred1='',y_pred2='',y_pred3= '',part='test',save_csv=True):\n",
        "\n",
        "            \n",
        "            pickle_in = open(partition_path,'rb')\n",
        "            partition = pickle.load(pickle_in)\n",
        "            pickle_in.close()\n",
        "            \n",
        "            pickle_in2 = open(labels_path1,'rb')\n",
        "            labels1 = pickle.load(pickle_in2)\n",
        "            pickle_in2.close()\n",
        "            \n",
        "            pickle_in2 = open(labels_path2,'rb')\n",
        "            labels2 = pickle.load(pickle_in2)\n",
        "            pickle_in2.close()\n",
        "            \n",
        "            pickle_in2 = open(labels_path3,'rb')\n",
        "            labels3 = pickle.load(pickle_in2)\n",
        "            pickle_in2.close()\n",
        "\n",
        "\n",
        "                     \n",
        "            print('Start evaluation...')\n",
        "            truE1=[]\n",
        "            truE2=[]\n",
        "            truE3=[]\n",
        "            truEname=[]\n",
        "            for im in partition[part]:\n",
        "                truE1.append(labels1[im])\n",
        "                truE2.append(labels2[im])\n",
        "                truE3.append(labels3[im])\n",
        "                truEname.append(im)\n",
        "\n",
        "            y_true1=np.array(truE1)\n",
        "            y_true2=np.array(truE2)\n",
        "            y_true3=np.array(truE3)           \n",
        "            y_truename=np.array(truEname)\n",
        "            \n",
        "\n",
        "            y_pred1=y_pred1.reshape(-1,)\n",
        "            y_pred2=y_pred2.reshape(-1,)\n",
        "            y_pred3=y_pred3.reshape(-1,)\n",
        "  \n",
        "\n",
        "\n",
        "            if (save_csv):\n",
        "                with open('test.csv', 'w') as f:\n",
        "                    fnames = ['name','pred blur', 'true blur', 'pred JPEG','true JPEG','pred noise','true noise']       \n",
        "                    writer = csv.DictWriter(f, fieldnames=fnames)\n",
        "                    writer.writeheader()\n",
        "\n",
        "                    for i in range(y_true1.size-1):\n",
        "                        pred1 = 0\n",
        "                        pred2 = 0\n",
        "                        pred3 = 0 \n",
        "                        for k in range(patches):\n",
        "                          pred1=y_pred1[(i*patches)+k]+pred1\n",
        "                        blur=pred1/patches\n",
        "                        \n",
        "                        for k in range(patches):\n",
        "                          pred2=y_pred2[(i*patches)+k]+pred2\n",
        "                        JPEG=pred2/patches\n",
        "                        \n",
        "                        for k in range(patches):\n",
        "                          pred3=y_pred3[(i*patches)+k]+pred3\n",
        "                        noise=pred3/patches                  \n",
        "                        \n",
        "                        writer.writerow({'name': y_truename[i],'pred blur' : blur, 'true blur': y_true1[i],'pred JPEG' : JPEG , 'true JPEG': y_true2[i], 'pred noise' : noise , 'true noise': y_true3[i]})\n",
        "\n",
        "            return True\n",
        "\n",
        "def test_model(save_csv=True, db='LIVEMD', max_pool=False, hiddenLayerDim=512):\n",
        "\n",
        "    dim=(224,224)\n",
        "    params = {'dim': dim,\n",
        "        'batch_size': 1,\n",
        "        'n_output': 1,\n",
        "        'n_channels': 3,\n",
        "        'shuffle': False,\n",
        "    }\n",
        "\n",
        "    if (db=='TID'):\n",
        "        test_generator=TID_GENERATOR(part='test', **params)\n",
        "\n",
        "    if (db=='CSIQ'):\n",
        "        test_generator = CSIQ_GENERATOR(part='test', **params)\n",
        "\n",
        "    if (db=='KADID'):\n",
        "        test_generator = KADID_GENERATOR(part='test', **params)\n",
        "\n",
        "    if (db=='LIVEMD'):\n",
        "        test_generator = LIVEMD_GENERATOR(part='test', **params)        \n",
        "\n",
        "\n",
        "    model = build_model (weights=None ,dropOutRate=0.25,hiddenLayerDim=512,num_denseLayer=2, input_shape=(224, 224, 3),fine_tune_all= False, max_pool = True)\n",
        "    adam=Adam(lr=0.0001)\n",
        "    losses = {\n",
        "            'Tower1': 'mean_squared_error',\n",
        "            'Tower2': 'mean_squared_error',\n",
        "            'Tower3': 'mean_squared_error',\n",
        "    }\n",
        "    lossWeights = {\n",
        "            'Tower1': 1.0, 'Tower2': 1.0\n",
        "            , 'Tower3':1.0\n",
        "  \n",
        "    }\n",
        "    metrics = {\n",
        "            'Tower1': 'accuracy', \n",
        "            'Tower2': 'accuracy',\n",
        "            'Tower3':'accuracy'\n",
        "    }\n",
        "\n",
        "    model.compile(optimizer=adam,\n",
        "              loss= losses,\n",
        "              loss_weights=lossWeights,\n",
        "              metrics=metrics)\n",
        "    \n",
        "    \n",
        "    model.load_weights('weights.h5')\n",
        "\n",
        "    y_pred1 , y_pred2, y_pred3 = model.predict_generator(generator=test_generator)\n",
        "    print(y_pred1)\n",
        "\n",
        "    if (db == \"TID\"):\n",
        "      return predictions(patches= 4 ,partition_path='partition_tid.pickle',labels_path1='blur_tid.pickle',labels_path2='jpeg_tid.pickle',labels_path3='noise_tid.pickle',y_pred1=y_pred1 ,y_pred2=y_pred2,y_pred3= y_pred3,part='test',save_csv=True)\n",
        "    if (db == \"CSIQ\"):\n",
        "      return predictions(patches= 4 ,partition_path='partition_csiq.pickle',labels_path1='blur_csiq.pickle',labels_path2='jpeg_tid.pickle',labels_path3='noise_tid.pickle',y_pred1=y_pred1,y_pred2=y_pred2,y_pred3=y_pred3,part='test',save_csv=True)\n",
        "    if (db == \"KADID\"):\n",
        "      return predictions(patches= 6 ,partition_path='partition_kadid.pickle',labels_path1='blur_kadid.pickle',labels_path2='jpeg_kadid.pickle',labels_path3='jpeg_kadid.pickle',y_pred1=y_pred1,y_pred2=y_pred2,y_pred3= y_pred3,part='test',save_csv=True)\n",
        "    if (db == \"LIVEMD\"):\n",
        "      return predictions(patches= 8 ,partition_path='partition_livemd.pickle',labels_path1='blur_livemd.pickle',labels_path2='jpeg_livemd.pickle',labels_path3='jpeg_livemd.pickle',y_pred1=y_pred1,y_pred2=y_pred2,y_pred3=y_pred3,part='test',save_csv=True)\n",
        "\n",
        "\n",
        "test_model(db='TID', hiddenLayerDim=512, save_csv=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oct_rjikZ-d2"
      },
      "source": [
        "### **Evaluate model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm8lalHuZ2rs"
      },
      "source": [
        "data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for i in range(len(data)):\n",
        "    pred = [round(data[\"pred blur\"][i]), round(data[\"pred JPEG\"][i]),round(data[\"pred noise\"][i])]\n",
        "    y_pred.append(pred)\n",
        "    true = [int(data[\"true blur\"][i]), int(data[\"true JPEG\"][i]),int(data[\"true noise\"][i])]\n",
        "    y_true.append(true)\n",
        "\n",
        "print('######### Accuracy #########')\n",
        "print(accuracy_score(y_true, y_pred))\n",
        "print('######### Precision BLUR #########')\n",
        "pred = round(data[\"pred blur\"])\n",
        "true = data[\"true blur\"]\n",
        "print(precision_score(true,pred))\n",
        "print('######### Precision JPEG #########')\n",
        "pred = round(data[\"pred JPEG\"])\n",
        "true = data[\"true JPEG\"]\n",
        "print(precision_score(true,pred))\n",
        "print('######### Precision NOISE #########')\n",
        "pred = round(data[\"pred noise\"])\n",
        "true = data[\"true noise\"]\n",
        "print(precision_score(true,pred))\n",
        "print('######### Recall BLUR #########')\n",
        "pred = round(data[\"pred blur\"])\n",
        "true = data[\"true blur\"]\n",
        "print(recall_score(true,pred))\n",
        "print('######### Recall JPEG #########')\n",
        "pred = round(data[\"pred JPEG\"])\n",
        "true = data[\"true JPEG\"]\n",
        "print(recall_score(true,pred))\n",
        "print('######### Recall NOISE #########')\n",
        "pred = round(data[\"pred noise\"])\n",
        "true = data[\"true noise\"]\n",
        "print(recall_score(true,pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}