{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_MTL_distortion_identification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMp5hN/F55p8SLIS5jMqd+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoubidaameur/Deep-Multi-Task-Learning-for-Image-Video-Distortions-Identification/blob/main/Deep_MTL_distortion_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QNHRDkmO6Uk"
      },
      "source": [
        "### **Deep Multi-Task Learning for Image/Video Distortions Identification**\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2v0QaKwO41I"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/pix2pix\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\r\n",
        "  </td>\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\r\n",
        "  </td>\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\r\n",
        "  </td>\r\n",
        "  <td>\r\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/pix2pix.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\r\n",
        "  </td>\r\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2s3cKD5PN97"
      },
      "source": [
        "This notebook demonstrates image distortions identification using deep multi-task learning. Using this technique you can identify and classify several distortion types using a single model simultaneously and accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv7Jy5UDPumP"
      },
      "source": [
        "### **Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPkfis4EP1P-"
      },
      "source": [
        "import os, sys\r\n",
        "import pickle\r\n",
        "import csv\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from skimage.util import view_as_windows\r\n",
        "from tensorflow.keras.layers import MaxPooling2D ,Dense ,Dropout, Flatten\r\n",
        "from tensorflow.keras.models import Model \r\n",
        "from tensorflow.keras.applications.densenet import DenseNet169\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.callbacks import Callback,TensorBoard\r\n",
        "\r\n",
        "from sklearn.metrics import accuracy_score, precision_score\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KdJFRxvRawj"
      },
      "source": [
        "### **Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uibeIGmRgCp"
      },
      "source": [
        "############## Uncomment the section of the desired dataset #####################\r\n",
        "\r\n",
        "\r\n",
        "####### TID-2013 #########\r\n",
        "# !wget \"http://www.ponomarenko.info/tid2013/tid2013.rar\"\r\n",
        "# !pip install unrar\r\n",
        "# !unrar x \"/content/tid2013.rar\"\r\n",
        "# mkdir /content/drive/MyDrive/tid\r\n",
        "# !mv /content/* /content/drive/MyDrive/tid\r\n",
        "\r\n",
        "\r\n",
        "####### KADID-10K #########\r\n",
        "# !wget \"https://datasets.vqa.mmsp-kn.de/archives/kadid10k.zip\"\r\n",
        "# !unzip /content/kadid10k.zip\r\n",
        "# mkdir /content/drive/MyDrive/kadid\r\n",
        "# mv /content/* /content/kadid\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "####### CSIQ #########*\r\n",
        "# !wget \"http://vision.eng.shizuoka.ac.jp/csiq/dst_imgs.zip\"\r\n",
        "# !unzip /content/dst_imgs.zip\r\n",
        "# mkdir /content/drive/MyDrive/csiq\r\n",
        "# mv /content/* /content/csiq\r\n",
        "\r\n",
        "####### LIVEMD #########*\r\n",
        "# !wget https://public.boxcloud.com/d/1/b1!IU4S1kNcRl9668x9nt0yijL48I6EGcI3qmccUX2YNXfVw4O2LNS4fEyI3x5aNXOL2OZWHNt-Z7vTEijwPWtsasa8_P2sdaE44u-7QR1N6cNOC3afB8Szq4biRIvtRNmLTnom6NZfdFNQSMbjG6g2yTbPpRoE1YuEGIT648tUedT_eDMHGEPDyINX4hOPrRV1CvIDYMqR4K7Oa0TrM689E8nF-RDRTH2ijx0PSDc84TxdORQ79XRpIq59K3-1OEkLvnDrpcPLxsZXiZAHNjrjggCjYNscJ83COC3_JUWgR6RQ_GpvoyB_60ba1b6o76mQ1UbFRnJ3snPCEuTxb_396uRdq4tEWrnf4G-dn5NKLdvofFiuXfFFEssLoRk3beeY10EuU7z-z6w2sB_3bgJnMysFwUleBBmEgk7zbizL6rtqZ6jxcRhzmGFD7JubS8sP_nQOrIo9JMbf95oMIfLQsom7A1LlgoSyeHJ23QTQuS1Syzjo7_iHE98jBJV3LgSRGsRLPCLfgbCEFmAQEWZ5qIHETx9FEsHtPMrCB8elqLLpfzYbRf5yq8_75sM7pn_Z2ardTDAOa_Uot-nP_rqVMTCHcJSjDPygX7wNwiGITIIQZrL5zX5LUXzCmRdGjgOCuATUIOKniWKrPRmd4lowJ5kJMHM4Gd87OifblNSMxxH1jyaViMT9z5cH8kPl2Eybl3SHTsmbiernufEAnKBaHk4YjEFFTgDaNMklhclyobRNiXJaem5IiO7qmgNhgOhbT0yAYPswQA19ufH0YHzEdEuXuQ1GW2ZqUsY-kojA_0Lz2kt-kWYgIUG8UpopB_YuB64n96icArMIbbDKdbXnsZVu0myUuEMOTkgMTWKsiCYsinvBwOD4b0l1kDtkBAsQToEU34us_xyloT_A1PoV_E6bKJPTdeaTpryWWe-RSM6T5eVnKWlRZqa73-T5WqMIkGLFTdRWJTGs1VlT_GZdFCqPR-VLg6jYvZQ7ju7Bps9defkzJ-r5VmZFEV6LFultyy2tUIEaNvRi6V1YNBHRGfTh-KBaN00B6wNaQcM1FprBRiTN6jixzInXuKTFTOnS7KxiarHX_7x1P19qlf9XU61puHYJIsTQU5CyjA2n5c__JpyQIx94cOykBtle4tA88m8V4CGBTFsNRqZoNpCC1rqh-r-gTiQKz7Tbc8ea4MQiIVd0TUAKFiXge2Thczjgl-_n0nD3M7dO3FiOZvAQNUxEonDGfjsoYGtMNHgoKQO6fH2y68joH26PDr0bdeSdW820M0N50DXIu4p8OJZRNB8mWcGqrCDWXEjq/download\r\n",
        "# !unrar x /content/drive/MyDrive/download -plivemultidistortiondatabase2013\r\n",
        "# mkdir /content/drive/MyDrive/livemd\r\n",
        "# !mv /content/drive/MyDrive/livemd/To_Release/Part1/blurjpeg/* /content/drive/MyDrive/livemd/\r\n",
        "# !mv /content/drive/MyDrive/livemd/To_Release/Part2/blurnoise/* /content/drive/MyDrive/livemd/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYzNhLuPTPFi"
      },
      "source": [
        "### **Data generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1cl4uA-TVMo"
      },
      "source": [
        "class generator_overlapping(tensorflow.keras.utils.Sequence):\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self):\r\n",
        "        self.on_epoch_end()\r\n",
        "    'Denotes the number of batches per epoch'\r\n",
        "    def __len__(self):\r\n",
        "      \r\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\r\n",
        "\r\n",
        "    def load_pkl(self,list_IDs_path,labels_path1,labels_path2,labels_path3, part):\r\n",
        "        pickle_in = open(list_IDs_path,'rb')\r\n",
        "        list_IDs = pickle.load(pickle_in)[part]\r\n",
        "        pickle_in.close()\r\n",
        "\r\n",
        "        pickle_in2 = open(labels_path1,'rb')\r\n",
        "        labels1 = pickle.load(pickle_in2)\r\n",
        "        pickle_in2.close()\r\n",
        "        \r\n",
        "        pickle_in2 = open(labels_path2,'rb')\r\n",
        "        labels2 = pickle.load(pickle_in2)\r\n",
        "        pickle_in2.close()\r\n",
        "        \r\n",
        "        pickle_in2 = open(labels_path3,'rb')\r\n",
        "        labels3 = pickle.load(pickle_in2)\r\n",
        "        pickle_in2.close()\r\n",
        "\r\n",
        "        return  list_IDs, labels1, labels2, labels3\r\n",
        "    \r\n",
        "    def loading_img(self):\r\n",
        "        return image.load_img(self.db_path+self.ID)\r\n",
        "\r\n",
        "    def init_y(self):\r\n",
        "        return np.empty((self.patches*self.batch_size,1), dtype=np.float32)\r\n",
        "    \r\n",
        "    def update_y1(self,ID):\r\n",
        "        return self.labels1[ID]\r\n",
        "    def update_y2(self,ID):\r\n",
        "        return self.labels2[ID]\r\n",
        "    def update_y3(self,ID):\r\n",
        "        return self.labels3[ID]\r\n",
        "    \r\n",
        "    \r\n",
        "    def update_x(self,x):\r\n",
        "        return x \r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        'Generate one batch of data'\r\n",
        "        # Generate indexes of the batch\r\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
        "        # Find list of IDs\r\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\r\n",
        "        # Generate data\r\n",
        "        X, y1, y2, y3 = self.__data_generation(list_IDs_temp)\r\n",
        "        return X, [y1, y2, y3]\r\n",
        "\r\n",
        "    def on_epoch_end(self):\r\n",
        "        'Updates indexes after each epoch'\r\n",
        "        self.indexes = np.arange(len(self.list_IDs))\r\n",
        "        if self.shuffle == True:\r\n",
        "            np.random.shuffle(self.indexes)\r\n",
        "\r\n",
        "    def ajust(self,img):\r\n",
        "        return img\r\n",
        "    def __data_generation(self, list_IDs_temp):\r\n",
        "        'Generates data containing batch_size samples' \r\n",
        "        # X : (n_samples, *dim, n_channels)\r\n",
        "        # Initialization\r\n",
        "        \r\n",
        "        X = np.empty((self.patches*self.batch_size, *self.dim, self.n_channels))\r\n",
        "        y1 =self.init_y() \r\n",
        "        y2 =self.init_y() \r\n",
        "        y3 = self.init_y()\r\n",
        "       \r\n",
        "        for i, ID in enumerate(list_IDs_temp):\r\n",
        "            self.ID=ID    \r\n",
        "            img = image.load_img(self.db_path+ID)\r\n",
        "            img = image.img_to_array(img)\r\n",
        "            img = applications.densenet.preprocess_input(img)\r\n",
        "            img=self.ajust(img)\r\n",
        "            x=view_as_windows(np.ascontiguousarray(img),(*self.dim,3),self.overlap_stride).reshape((-1,*self.dim,3))      \r\n",
        "\r\n",
        "            X[(i)*self.patches :(i+1)*self.patches,:,:,:]=self.update_x(x)\r\n",
        "            y1[(i)*self.patches :(i+1)*self.patches]=self.update_y1(ID)\r\n",
        "            y2[(i)*self.patches :(i+1)*self.patches]=self.update_y2(ID)\r\n",
        "            y3[(i)*self.patches :(i+1)*self.patches]=self.update_y3(ID)\r\n",
        "        return X, y1, y2, y3\r\n",
        "\r\n",
        "   \r\n",
        "    \r\n",
        "class LIVEMD_GENERATOR(generator_overlapping):\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\r\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\r\n",
        "        self.base=base\r\n",
        "        self.dim = dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.n_output = n_output\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\r\n",
        "        self.db_path='/content/drive/MyDrive/tid/distorted_images/'\r\n",
        "        list_IDs_path='/content/drive/MyDrive/tid/files/partition_tid1.pickle'\r\n",
        "        labels_path1='/content/drive/MyDrive/tid/files/blur_tid.pickle'\r\n",
        "        labels_path2='/content/drive/MyDrive/tid/files/jpeg_tid.pickle'\r\n",
        "        labels_path3 = '/content/drive/MyDrive/tid/files/noise_tid.pickle'\r\n",
        "        self.patches=8  \r\n",
        "        self.overlap_stride = 350\r\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "class TID_GENERATOR(generator_overlapping):\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\r\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\r\n",
        "        self.base=base\r\n",
        "        self.dim = dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.n_output = n_output\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\r\n",
        "        self.db_path='/content/drive/MyDrive/tid/distorted_images/'\r\n",
        "        list_IDs_path='/content/drive/MyDrive/tid/files/partition_tid1.pickle'\r\n",
        "        labels_path1='/content/drive/MyDrive/tid/files/blur_tid.pickle'\r\n",
        "        labels_path2='/content/drive/MyDrive/tid/files/jpeg_tid.pickle'\r\n",
        "        labels_path3 = '/content/drive/MyDrive/tid/files/noise_tid.pickle'\r\n",
        "        self.patches=4  \r\n",
        "        self.overlap_stride = 150\r\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "class CSIQ_GENERATOR(generator_overlapping):\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\r\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\r\n",
        "        self.base=base\r\n",
        "        self.dim = dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.n_output = n_output\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\r\n",
        "        self.db_path='/content/drive/MyDrive/tid/distorted_images/'\r\n",
        "        list_IDs_path='/content/drive/MyDrive/tid/files/partition_tid1.pickle'\r\n",
        "        labels_path1='/content/drive/MyDrive/tid/files/blur_tid.pickle'\r\n",
        "        labels_path2='/content/drive/MyDrive/tid/files/jpeg_tid.pickle'\r\n",
        "        labels_path3 = '/content/drive/MyDrive/tid/files/noise_tid.pickle'\r\n",
        "        self.patches=4  \r\n",
        "        self.overlap_stride = 160\r\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "class KADID_GENERATOR(generator_overlapping):\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self,batch_size=1, dim=(224,224), n_channels=3,\r\n",
        "                 n_output=1, shuffle=True,part='complete',base='vgg19'):\r\n",
        "        self.base=base\r\n",
        "        self.dim = dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.n_channels = n_channels\r\n",
        "        self.n_output = n_output\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.input_dim=(self.dim[0],self.dim[1],self.n_channels)\r\n",
        "        self.db_path='/content/drive/MyDrive/tid/distorted_images/'\r\n",
        "        list_IDs_path='/content/drive/MyDrive/tid/files/partition_tid1.pickle'\r\n",
        "        labels_path1='/content/drive/MyDrive/tid/files/blur_tid.pickle'\r\n",
        "        labels_path2='/content/drive/MyDrive/tid/files/jpeg_tid.pickle'\r\n",
        "        labels_path3 = '/content/drive/MyDrive/tid/files/noise_tid.pickle'\r\n",
        "        self.patches=6  \r\n",
        "        self.overlap_stride = 100\r\n",
        "        self.list_IDs,self.labels1, self.labels2, self.labels3 =super().load_pkl(list_IDs_path,labels_path1, labels_path2, labels_path3, part)\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9hTh0EdU_MH"
      },
      "source": [
        "### **Build the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YvhIqQdVEpq"
      },
      "source": [
        "def build_model(max_pool= False ,weights='imagenet', dropOutRate=0.25,hiddenLayerDim=512,num_denseLayer=2,input_shape = (224,224,3), include_top = False,fine_tune_all = False, num_towers =2): \r\n",
        "    \r\n",
        "    base_model = DenseNet169(weights=weights ,include_top = include_top, input_shape = input_shape)\r\n",
        "\r\n",
        "    if (fine_tune_all ==False):\r\n",
        "        for layer in base_model.layers:\r\n",
        "            layer.trainable = False\r\n",
        "            \r\n",
        "    x =base_model.layers[-1].output\r\n",
        "        \r\n",
        "    if (max_pool):\r\n",
        "        x= MaxPooling2D(pool_size=(2,2))(x)\r\n",
        "    x =Flatten()(x)\r\n",
        "    features = x\r\n",
        "    for i in range(num_denseLayer):\r\n",
        "        features = Dense(hiddenLayerDim, activation='relu',name=\"DenseTower1\"+str(i))(features)\r\n",
        "        features = Dropout(dropOutRate, name=\"DropoutTower1\"+ str(i))(features)\r\n",
        "    output1 = Dense(1, name=\"Tower1\", activation=\"sigmoid\")(features)\r\n",
        "        \r\n",
        "       \r\n",
        "    features = x\r\n",
        "    for i in range(num_denseLayer):\r\n",
        "        features = Dense(hiddenLayerDim, activation='relu',name=\"DenseTower2\"+str(i))(features)\r\n",
        "        features = Dropout(dropOutRate, name=\"DropoutTower2\"+ str(i))(features)\r\n",
        "    output2 = Dense(1, name= \"Tower2\", activation=\"sigmoid\")(features)\r\n",
        "             \r\n",
        "    features = x\r\n",
        "    for i in range(num_denseLayer):\r\n",
        "        features = Dense(hiddenLayerDim, activation='relu',name=\"DenseTower3\"+str(i))(features)\r\n",
        "        features = Dropout(dropOutRate, name=\"DropoutTower3\"+ str(i))(features)\r\n",
        "    output3 = Dense(1, name= \"Tower3\", activation=\"sigmoid\")(features)\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "    model = Model(inputs=base_model.layers[0].output, outputs= [output1, output2, output3])\r\n",
        "\r\n",
        "    return model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vs-yR68V2fT"
      },
      "source": [
        "### **Train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyzrsI1ZV-VO"
      },
      "source": [
        "def Training(batch_size=8,db='CSIQ',dropOutRate=0.25,hiddenLayerDim=512,epochs=60, num_denseLayer=2,fine_tune_all=False):\r\n",
        "\r\n",
        "\r\n",
        "    #Define parameters \r\n",
        "    params = {'dim':(224,224),\r\n",
        "        'batch_size': batch_size,\r\n",
        "        'n_output': 1,\r\n",
        "        'n_channels': 3,\r\n",
        "        'shuffle': True,\r\n",
        "    }\r\n",
        "\r\n",
        "    if (db=='TID'):\r\n",
        "        training_generator = TID_GENERATOR(part='train', **params)\r\n",
        "        validation_generator =TID_GENERATOR(part='test', **params)\r\n",
        "\r\n",
        "    if (db=='CSIQ'):\r\n",
        "        training_generator = CSIQ_GENERATOR(part='train', **params)\r\n",
        "        validation_generator = CSIQ_GENERATOR(part='test', **params)\r\n",
        "\r\n",
        "    if (db=='KADID'):\r\n",
        "        training_generator = KADID_GENERATOR(part='train', **params)\r\n",
        "        validation_generator = KADID_GENERATOR(part='test', **params)\r\n",
        "\r\n",
        "    if (db=='LIVEMD'):\r\n",
        "        training_generator = LIVEMD_GENERATOR(part='train', **params)\r\n",
        "        validation_generator = LIVEMD_GENERATOR(part='test', **params)        \r\n",
        "\r\n",
        "\r\n",
        "    model = build_model (weights='imagenet',dropOutRate=0.25,hiddenLayerDim=512,num_denseLayer=2, input_shape=(224, 224, 3),fine_tune_all= False, max_pool = True)\r\n",
        "    model.summary()\r\n",
        "    adam=Adam(lr=0.0001)\r\n",
        "    losses = {\r\n",
        "            'Tower1': 'binary_crossentropy',\r\n",
        "            'Tower2': 'binary_crossentropy',\r\n",
        "            'Tower3': 'binary_crossentropy'\r\n",
        "    }\r\n",
        "    lossWeights = {\r\n",
        "            'Tower1': 1.0, \r\n",
        "            'Tower2': 1.0,\r\n",
        "            'Tower3':1.0\r\n",
        "    }\r\n",
        "    metrics = {\r\n",
        "            'Tower1': 'accuracy', \r\n",
        "            'Tower2': 'accuracy',\r\n",
        "            'Tower3':'accuracy'\r\n",
        "    }\r\n",
        "\r\n",
        "    model.compile(optimizer=adam,\r\n",
        "              loss= losses,\r\n",
        "              loss_weights=lossWeights,\r\n",
        "              metrics=metrics)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,write_graph=True, write_images=False)\r\n",
        "    callbacks = tensorflow.keras.callbacks.ModelCheckpoint('weights.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\r\n",
        "\r\n",
        "    print('Start training of '+ training_type)\r\n",
        "    history =model.fit_generator(generator=training_generator,\r\n",
        "                                validation_data=validation_generator,\r\n",
        "                                use_multiprocessing=True,\r\n",
        "                                workers=4,\r\n",
        "                                epochs=epochs,\r\n",
        "                                callbacks= callbacks\r\n",
        "                               )\r\n",
        "\r\n",
        "    print('Training is finished')\r\n",
        "\r\n",
        "    return True\r\n",
        "\r\n",
        "\r\n",
        "Training(db='TID',batch_size=6, hiddenLayerDim=512,num_denseLayer=2,dropOutRate=0.25 ,fine_tune_all=False ,epochs=60, save_json=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QVzjNiYX1wL"
      },
      "source": [
        "### **Test model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFjStau8X1NO"
      },
      "source": [
        "def predictions(patches= 4 ,partition_path='',labels_path1='',labels_path2='',labels_path3='',y_pred1='',y_pred2='',y_pred3= '',part='test',save_csv=True):\r\n",
        "\r\n",
        "            \r\n",
        "            pickle_in = open(partition_path,'rb')\r\n",
        "            partition = pickle.load(pickle_in)\r\n",
        "            pickle_in.close()\r\n",
        "            \r\n",
        "            pickle_in2 = open(labels_path1,'rb')\r\n",
        "            labels1 = pickle.load(pickle_in2)\r\n",
        "            pickle_in2.close()\r\n",
        "            \r\n",
        "            pickle_in2 = open(labels_path2,'rb')\r\n",
        "            labels2 = pickle.load(pickle_in2)\r\n",
        "            pickle_in2.close()\r\n",
        "            \r\n",
        "            pickle_in2 = open(labels_path3,'rb')\r\n",
        "            labels3 = pickle.load(pickle_in2)\r\n",
        "            pickle_in2.close()\r\n",
        "\r\n",
        "\r\n",
        "                     \r\n",
        "            print('Start evaluation...')\r\n",
        "            truE1=[]\r\n",
        "            truE2=[]\r\n",
        "            truE3=[]\r\n",
        "            truEname=[]\r\n",
        "            for im in partition[part]:\r\n",
        "                truE1.append(labels1[im])\r\n",
        "                truE2.append(labels2[im])\r\n",
        "                truE3.append(labels3[im])\r\n",
        "                truEname.append(im)\r\n",
        "\r\n",
        "            y_true1=np.array(truE1)\r\n",
        "            y_true2=np.array(truE2)\r\n",
        "            y_true3=np.array(truE3)           \r\n",
        "            y_truename=np.array(truEname)\r\n",
        "            \r\n",
        "\r\n",
        "            y_pred1=y_pred1.reshape(-1,)\r\n",
        "            y_pred2=y_pred2.reshape(-1,)\r\n",
        "            y_pred3=y_pred3.reshape(-1,)\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "            if (save_csv):\r\n",
        "                with open('test.csv', 'w') as f:\r\n",
        "                    fnames = ['name','pred blur', 'true blur', 'pred JPEG','true JPEG','pred noise','true noise']       \r\n",
        "                    writer = csv.DictWriter(f, fieldnames=fnames)\r\n",
        "                    writer.writeheader()\r\n",
        "\r\n",
        "                    for i in range(y_true1.size-1):\r\n",
        "                        pred1 = 0\r\n",
        "                        pred2 = 0\r\n",
        "                        pred3 = 0 \r\n",
        "                        for k in range(patches):\r\n",
        "                          pred1=y_pred1[(i*patches)+k]+pred1\r\n",
        "                        blur=pred1/patches\r\n",
        "                        \r\n",
        "                        for k in range(patches):\r\n",
        "                          pred2=y_pred2[(i*patches)+k]+pred2\r\n",
        "                        JPEG=pred2/patches\r\n",
        "                        \r\n",
        "                        for k in range(patches):\r\n",
        "                          pred3=y_pred3[(i*patches)+k]+pred3\r\n",
        "                        noise=pred3/patches                  \r\n",
        "                        \r\n",
        "                        writer.writerow({'name': y_truename[i],'pred blur' : blur, 'true blur': y_true1[i],'pred JPEG' : JPEG , 'true JPEG': y_true2[i], 'pred noise' : noise , 'true noise': y_true3[i]})\r\n",
        "\r\n",
        "            return True\r\n",
        "\r\n",
        "def test_model(save_csv=True, db='LIVEMD', max_pool=False, hiddenLayerDim=512):\r\n",
        "\r\n",
        "    dim=(224,224)\r\n",
        "    params = {'dim': dim,\r\n",
        "        'batch_size': 1,\r\n",
        "        'n_output': 1,\r\n",
        "        'n_channels': 3,\r\n",
        "        'shuffle': False,\r\n",
        "    }\r\n",
        "\r\n",
        "    if (db=='TID'):\r\n",
        "        test_generator =TID_GENERATOR(part='test', **params)\r\n",
        "\r\n",
        "    if (db=='CSIQ'):\r\n",
        "        test_generator = CSIQ_GENERATOR(part='test', **params)\r\n",
        "\r\n",
        "    if (db=='KADID'):\r\n",
        "        test_generator = KADID_GENERATOR(part='test', **params)\r\n",
        "\r\n",
        "    if (db=='LIVEMD'):\r\n",
        "        test_generator = LIVEMD_GENERATOR(part='test', **params)        \r\n",
        "\r\n",
        "\r\n",
        "    model = build_model (weights=None, dropOutRate=0.25, hiddenLayerDim=512, num_denseLayer=2, input_shape=(224, 224, 3))\r\n",
        "    adam=Adam(lr=0.0001)\r\n",
        "    losses = {\r\n",
        "            'Tower1': 'mean_squared_error',\r\n",
        "            'Tower2': 'mean_squared_error',\r\n",
        "            'Tower3': 'mean_squared_error',\r\n",
        "    }\r\n",
        "    lossWeights = {\r\n",
        "            'Tower1': 1.0, 'Tower2': 1.0\r\n",
        "            , 'Tower3':1.0\r\n",
        "  \r\n",
        "    }\r\n",
        "    metrics = {\r\n",
        "            'Tower1': 'accuracy', \r\n",
        "            'Tower2': 'accuracy',\r\n",
        "            'Tower3':'accuracy'\r\n",
        "    }\r\n",
        "\r\n",
        "    model.compile(optimizer=adam,\r\n",
        "              loss= losses,\r\n",
        "              loss_weights=lossWeights,\r\n",
        "              metrics=metrics)\r\n",
        "    \r\n",
        "    \r\n",
        "    model.load_weights('weights.h5')\r\n",
        "\r\n",
        "    y_pred1 , y_pred2, y_pred3 = model.predict_generator(generator=test_generator)\r\n",
        "\r\n",
        "    if (db == \"TID\"):\r\n",
        "      return predictions(patches= 4 ,partition_path='',labels_path1='',labels_path2='',labels_path3='',y_pred1='',y_pred2='',y_pred3= '',part='test',save_csv=True)\r\n",
        "    if (db == \"CSIQ\"):\r\n",
        "      return predictions(patches= 4 ,partition_path='',labels_path1='',labels_path2='',labels_path3='',y_pred1='',y_pred2='',y_pred3= '',part='test',save_csv=True)\r\n",
        "    if (db == \"KADID\"):\r\n",
        "      return predictions(patches= 6 ,partition_path='',labels_path1='',labels_path2='',labels_path3='',y_pred1='',y_pred2='',y_pred3= '',part='test',save_csv=True)\r\n",
        "    if (db == \"LIVEMD\"):\r\n",
        "      return predictions(patches= 8 ,partition_path='',labels_path1='',labels_path2='',labels_path3='',y_pred1='',y_pred2='',y_pred3= '',part='test',save_csv=True)\r\n",
        "\r\n",
        "\r\n",
        "test_model(db='LIVEMD', hiddenLayerDim=512, save_csv=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oct_rjikZ-d2"
      },
      "source": [
        "### **Evaluate test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm8lalHuZ2rs"
      },
      "source": [
        "data = pd.read_csv(\"test.csv\")\r\n",
        "\r\n",
        "\r\n",
        "y_pred = []\r\n",
        "y_true = []\r\n",
        "for i in range(len(data)):\r\n",
        "    pred = [round(data[\"pred blur\"][i]), round(data[\"pred JPEG\"][i]),round(data[\"pred noise\"][i])]\r\n",
        "    y_pred.append(pred)\r\n",
        "    true = [int(data[\"true blur\"][i]), int(data[\"true JPEG\"][i]),int(data[\"true noise\"][i])]\r\n",
        "    y_true.append(true)\r\n",
        "\r\n",
        "print('######### Accuracy #########')\r\n",
        "print(accuracy_score(y_true, y_pred))\r\n",
        "print('######### Precision BLUR #########')\r\n",
        "pred = round(data[\"pred blur\"])\r\n",
        "true = data[\"true blur\"]\r\n",
        "print(precision_score(true,pred))\r\n",
        "print('######### Precision JPEG #########')\r\n",
        "pred = round(data[\"pred JPEG\"])\r\n",
        "true = data[\"true JPEG\"]\r\n",
        "print(precision_score(true,pred))\r\n",
        "print('######### Precision NOISE #########')\r\n",
        "pred = round(data[\"pred noise\"])\r\n",
        "true = data[\"true noise\"]\r\n",
        "print(precision_score(true,pred))\r\n",
        "print('######### Recall BLUR #########')\r\n",
        "pred = round(data[\"pred blur\"])\r\n",
        "true = data[\"true blur\"]\r\n",
        "print(recall_score(true,pred))\r\n",
        "print('######### Recall JPEG #########')\r\n",
        "pred = round(data[\"pred JPEG\"])\r\n",
        "true = data[\"true JPEG\"]\r\n",
        "print(recall_score(true,pred))\r\n",
        "print('######### Recall NOISE #########')\r\n",
        "pred = round(data[\"pred noise\"])\r\n",
        "true = data[\"true noise\"]\r\n",
        "print(recall_score(true,pred))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}